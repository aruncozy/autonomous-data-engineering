"""Auto-generated Apache Beam batch pipeline for {{ source_path }}."""

import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions


def run():
    options = PipelineOptions(
        runner="DataflowRunner",
        project="{{ project_id | default('my-project') }}",
        region="{{ region | default('us-central1') }}",
        temp_location="gs://{{ temp_bucket | default('temp-bucket') }}/beam-temp/",
        staging_location="gs://{{ temp_bucket | default('temp-bucket') }}/beam-staging/",
    )

    table_schema = {
        "fields": [
            {% for col in columns %}
            {"name": "{{ col }}", "type": "STRING", "mode": "NULLABLE"},
            {% endfor %}
        ]
    }

    with beam.Pipeline(options=options) as p:
        {% if format == "csv" %}
        rows = (
            p
            | "ReadCSV" >> beam.io.ReadFromText("{{ source_path }}", skip_header_lines=1)
            | "ParseCSV" >> beam.Map(parse_csv)
        )
        {% elif format == "json" %}
        rows = (
            p
            | "ReadJSON" >> beam.io.ReadFromText("{{ source_path }}")
            | "ParseJSON" >> beam.Map(lambda line: __import__("json").loads(line))
        )
        {% elif format == "parquet" %}
        rows = (
            p
            | "ReadParquet" >> beam.io.ReadFromParquet("{{ source_path }}")
        )
        {% else %}
        rows = (
            p
            | "ReadText" >> beam.io.ReadFromText("{{ source_path }}")
            | "Parse" >> beam.Map(lambda line: {"raw": line})
        )
        {% endif %}

        cleaned = (
            rows
            | "FilterNulls" >> beam.Filter(lambda row: row is not None)
            | "StandardizeTypes" >> beam.Map(standardize)
        )

        _ = (
            cleaned
            | "WriteToBQ" >> beam.io.WriteToBigQuery(
                "{{ target_table }}",
                schema=table_schema,
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
            )
        )


def parse_csv(line: str) -> dict:
    """Parse a CSV line into a dict keyed by column names."""
    columns = [{% for col in columns %}"{{ col }}", {% endfor %}]
    values = line.split(",")
    return dict(zip(columns, values))


def standardize(row: dict) -> dict:
    """Apply basic type standardization."""
    cleaned = {}
    for k, v in row.items():
        if v is None or (isinstance(v, str) and v.strip() == ""):
            cleaned[k] = None
        else:
            cleaned[k] = str(v).strip()
    return cleaned


if __name__ == "__main__":
    run()
